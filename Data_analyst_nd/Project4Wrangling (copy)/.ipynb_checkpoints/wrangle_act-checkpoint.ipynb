{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Project: Wrangle and analyze data </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy as tw #API Tweeter\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Phase 1: Gather the data </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Definition: Define the objectives </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<li>\n",
    "<ul> Download the archive</ul>\n",
    "<ul>Download the additional data via Tweepy </ul>\n",
    "<ul>Consolidate the data for the next step </ul>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Read the csv archive file</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('twitter-archive-enhanced.csv' , encoding ='utf-8' , sep=',')\n",
    "#df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Download programmitically the file from the url</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code: implement the steps  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make directory if it doesn't already exist\n",
    "folder_name = 'dog_ratings'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    \n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'dog_ratings/image-predictions.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-77a1619cb072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m with open(os.path.join(folder_name,\n\u001b[0;32m----> 2\u001b[0;31m                       url.split('/')[-1]), mode ='wb') as file:\n\u001b[0m\u001b[1;32m      3\u001b[0m      \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'dog_ratings/image-predictions.tsv'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(folder_name,\n",
    "                      url.split('/')[-1]), mode ='wb') as file:\n",
    "     file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(folder_name+\"/\"'image-predictions.tsv', sep='\t')\n",
    "df_pred.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Fetching data from Tweeter</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consumer_key= 'yourkeyhere'\n",
    "#consumer_secret= 'yourkeyhere'\n",
    "\n",
    "consumer_key = 'gVctStkiNVGRZVODXoD6ArBOU'\n",
    "\n",
    "consumer_secret ='wCRLAlmCtA1InfqbNpsH54BVitMH1EYltwjl7a2CD4rn8ATy6E'\n",
    "\n",
    "access_token= '3338026065-ELz2REKz16xxogbWDDmeq77btntm1x0OnYNNsca'\n",
    "access_token_secret= 'rRjTTxHORIiyH81ncNwQLZyHeerg9bcMuAZH0k4MzBNzX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test Api</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search term and the date_since date as variables\n",
    "#search_words = \"#wildfires\"\n",
    "search_words = \"@dog_rates\"\n",
    "date_since = \"2017-08-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tweets\n",
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(50)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and print tweets\n",
    "for tweet in tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tweeter API</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n",
    "# df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n",
    "# change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n",
    "# NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "# Twitter API code was sent to this student from a Udacity instructor\n",
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "df_1 = df.copy()\n",
    "tweet_ids = df_1.tweet_id.values\n",
    "len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_json.txt') as file:\n",
    "    status = []\n",
    "    count = 0\n",
    "    for line in file:\n",
    "        print(count)\n",
    "        status.append(json.loads(line))\n",
    "        count = count +1\n",
    "print('read terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api =  pd.DataFrame(status)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test:  test and verify  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('df.csv', index=False)\n",
    "#df_api.to_csv('df_api.csv', index=False)\n",
    "#df_pred.to_csv('df_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Reloading if necessary </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('df.csv')\n",
    "#df_pred = pd.read_csv('df_pred.csv')\n",
    "#df_api = pd.read_csv('df_api.csv')\n",
    "#read to avoid query the twitter api\n",
    "print('frames reloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase 2: Assess the data programmatically </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Assess the tidiness of the sets</li>\n",
    "<li>Assess data quality issues: missing values, types, consistence, duplicates</li>\n",
    "<li>Summarize the relevant issues before the cleaning phase</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Completeness: do we have all of the records that we should? Do we have missing records or not? Are there specific rows, columns, or cells missing?\n",
    "Validity: we have the records, but they're not valid, i.e., they don't conform to a defined schema. A schema is a defined set of rules for data. These rules can be real-world constraints (e.g. negative height is impossible) and table-specific constraints (e.g. unique key constraints in tables).\n",
    "Accuracy: inaccurate data is wrong data that is valid. It adheres to the defined schema, but it is still incorrect. Example: a patient's weight that is 5 lbs too heavy because the scale was faulty.\n",
    "Consistency: inconsistent data is both valid and accurate, but there are multiple correct ways of referring to the same thing. Consistency, i.e., a standard format, in columns that represent the same data across tables and/or within tables is desired.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Assessing the types </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(df.columns):\n",
    "    print(i, v , type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(df_api.columns):\n",
    "    print(i, v , type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(df_pred.columns):\n",
    "    print(i, v , type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualizing missing values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = df.isna().sum()\n",
    "base_color = sns.color_palette()[0]\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot( na_counts,na_counts.index.values, color = base_color);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = df_pred.isna().sum()\n",
    "base_color = sns.color_palette()[0]\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot( na_counts,na_counts.index.values, color = base_color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data extracted from the api seem incomplete. There are a lot of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = df_api.isna().sum()\n",
    "base_color = sns.color_palette()[0]\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot( na_counts,na_counts.index.values, color = base_color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "print('----------------------------')\n",
    "print(df_pred.count())\n",
    "print('----------------------------')\n",
    "print(df_api.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Consolidate the data in one or maximal two dataframes</li>\n",
    "<li>Select the required columns</li>\n",
    "<li> Filter the retweets </li>\n",
    "<li>Handle the missing values</li>\n",
    "<li>The data extracted from the tweeter api are not tidy </li>\n",
    "<li>The user column of the api dataframe contains id and id<-str</li>\n",
    "<li>The extended_entities column contains two fields  </li>\n",
    "<li>The date fields should be treated as such  </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase 3: Cleaning the data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define</h3>\n",
    "<p>Based on the issues listed below i clean the dataset </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>I correct the date types of the dataframes.</li>\n",
    "<li>There is no need to handle missing values since this is going to affect the integrity of the data.</li>\n",
    "<li>I create a copy of the three dataframes before starting the cleaning phase</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to reuse\n",
    "def parse_date(dateFrame):\n",
    "    dateFrame = pd.to_datetime(dateFrame)\n",
    "    return dateFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "df_api_clean = df_api.copy()\n",
    "df_pred_clean = df_pred.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean =df_clean[['tweet_id','timestamp','source','expanded_urls','rating_numerator','rating_denominator','name','pupper']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type correction for date\n",
    "df_clean['timestamp'] = parse_date(df_clean['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.merge(df_clean, df_pred_clean, left_on ='tweet_id', right_on ='tweet_id' , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I have 2356 tweets after the merging of the read csv and the prediction file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I need to choose which attributes to extract from the tweeter api dataframe.\n",
    "> I also need to extract the user_id from the user column which contains duplicated informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = df_api_clean.isna().sum()\n",
    "base_color = sns.color_palette()[0]\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot( na_counts,na_counts.index.values, color = base_color);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_clean.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_clean = df_api_clean[['created_at','entities','favorite_count','id','is_quote_status','retweet_count','retweeted','source','user','truncated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type correction for created_at\n",
    "df_api_clean['created_at'] = parse_date(df_api_clean['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_clean['user_id']  =df_api_clean['user'].str.extract('(\\d+)')\n",
    "df_api_clean.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_clean = df_api_clean[['created_at','favorite_count','id','is_quote_status','retweet_count','retweeted','source','user_id','truncated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_clean.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api = pd.merge(res, df_api_clean, left_on ='tweet_id', right_on ='id' , how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have consolidated all information in a dataframe</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>documentation of analysis and insights into the data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Exploring with the hist function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.hist(figsize=(12,12));"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Filtering out the first retweeted\n",
    "res_api = res_api[res_api['retweeted']== False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature engineering </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rating column based on the division\n",
    "res_api['rating'] = res_api['rating_numerator'] / res_api['rating_denominator']\n",
    "res_api.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Summarizing the Numerical data</p>\n",
    "<ul>\n",
    "<li> The rating numerator  </li>\n",
    "<li>The rating denumerator</li>\n",
    "<li>The favorite count</li>\n",
    "<li>The retweet count  </li>\n",
    "<li> p1_conf is how confident the algorithm is in its  prediction   </li>\n",
    "<li> p2_conf is how confident the algorithm is in its  prediction   </li>\n",
    "<li> p3_conf is how confident the algorithm is in its  prediction   </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Summarizing the categorical data </p>\n",
    "<ul>\n",
    "<li>The name  ; ) </li>\n",
    "<li>The predicted classes: p1, p2, p3    </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api['p1_dog'].astype(bool)\n",
    "res_api['p2_dog'].astype(bool)\n",
    "res_api['p3_dog'].astype(bool)\n",
    "res_api['dogOrNot']= res_api[res_api['p1_dog'] | res_api['p2_dog'] | res_api['p3_dog'] ]\n",
    "res_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Returning the class with the highest confidence. For that I compare the confidence returnes by each algorithm</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returning the maximal confidence\n",
    "res_api['max_conf'] =res_api[['p1_conf','p2_conf','p3_conf']].max(axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We need to consider that some classes specified that the picture is not one of a dog . Some tweets are simply not classified. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api_p1 = res_api.query('p1_conf == max_conf').query('p1_dog == True')\n",
    "res_api_p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api_p2 = res_api.query('p2_conf == max_conf').query('p2_dog == True')\n",
    "res_api_p2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api_p3 = res_api.query('p3_conf == max_conf').query('p3_dog == True')\n",
    "res_api_p3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The first algorithm is the most reliable according to the level of confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What are the most retweeted classes of dogs ?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_api.groupby('p1' ,as_index = False)['retweet_count'].count().plot(kind ='bar', title ='')\n",
    "#plt.xlabel('')\n",
    "res_api_p1.groupby('p1' ,as_index = False)['retweet_count'].count().sort_values(by=['retweet_count'] , ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> According to p1 the most retweeted sorts of dogs are: </p>\n",
    "<ul>\n",
    "<li>golden_retriever</li>\n",
    "<li>Labrador_retriever</li>\n",
    "<li>Pembroke</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The less popular dogs:</p>\n",
    "    <ul>\n",
    "    <li> Scotch_terrier  </li>\n",
    "     <li>  clumber </li>\n",
    "        <li> Japanese_spaniel</li>\n",
    "    </ul>\n",
    "<p> These dog species are possibly rare or simply unpopular. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What types of dogs do have on average the best rating ?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api_p1.groupby('p1' ,as_index = False)['rating'].mean().sort_values(by=['rating'] , ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The dogs which have the best rating on average are:\n",
    "</p>\n",
    "<ul>\n",
    "<li>clumber  </li>\n",
    "<li>  Pomeranian</li>\n",
    "<li> Saluki </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api_final = res_api.drop(['expanded_urls', 'p3','p3_conf','source_y','jpg_url','created_at','id','source_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_api['pupper'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sns.color_palette()[0]\n",
    "sns.countplot(data = res_api, x = 'pupper', color = base_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
